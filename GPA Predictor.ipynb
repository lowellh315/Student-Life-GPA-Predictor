{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from numpy import *\n",
    "import glob\n",
    "import scipy \n",
    "from datetime import datetime as dt\n",
    "import sklearn\n",
    "sns.style = 'darkgrid'\n",
    "import ruptures as rpt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregating Features\n",
    "\n",
    "The following cells work to aggregate a lot of sensor data and them add additional data for each user's slope for that data throughout the term. The files we used were taken from the following github: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_doy(stri): \n",
    "    \"\"\"\n",
    "    given an input string, returns the datetime of that string in Y-m-d format. \n",
    "    \"\"\"\n",
    "    date = dt.strptime(stri, '%Y-%m-%d')\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "def linear_coef(X, Y): \n",
    "    \"\"\"\n",
    "    given pandas series X and Y, returns the overall slope of those series over the course of the term. this is used to \n",
    "    find behavioral slopes for time periods, where X is time and y is a behavior. \n",
    "    \"\"\"\n",
    "    x_2 = X.values.reshape(-1, 1)\n",
    "    model = LinearRegression()\n",
    "    model.fit(x_2, Y)\n",
    "    # commented section plots slope\n",
    "    #data = pd.DataFrame({'x': X, 'y': Y})\n",
    "    #sns.regplot(x='x', y='y', data = data, ci = None)\n",
    "    return model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear_coef(bedtime[bedtime['uid'] == 'u00'].day, bedtime[bedtime['uid'] == 'u00'].night_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(times_tuple):\n",
    "    \"\"\"\n",
    "    input: tuple containing start and end times (in hours on 24 hour scale)\n",
    "    output: the epoch that corresponds to the timestamps\n",
    "    note: we chose to only return timestamps that had both start and end time within a single epoch. An alternative would\n",
    "    be splitting conversations that span multiple epochs into two conversations, one in each epoch, but we decided not to \n",
    "    because that would double count conversations for each user.  \n",
    "    \"\"\"\n",
    "    start = times_tuple[0]\n",
    "    end = times_tuple[1]\n",
    "    \n",
    "    # Day epoch: hours 10 am -6 pm\n",
    "    if (start and end) >= 10 and (start and end) <18 :\n",
    "        return 'Day'\n",
    "    # Night epoch: 12 am - 10 am\n",
    "    elif (start and end)<10:\n",
    "        return 'Night'\n",
    "    # evening epoch: 6 pm - 12 am\n",
    "    elif (start and end) >= 18:\n",
    "        return 'Evening'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells aggregate data from various sensors in a format including the day of the year and the column of interest renamed to a more descriptive name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sensors(): \n",
    "    # grades data\n",
    "    grades = pd.read_csv('tables/grades/grades.csv')\n",
    "\n",
    "    # sleep duration data\n",
    "    bedtime = pd.read_csv(\"tables/bedtime/bedtime.csv\")\n",
    "    bedtime['day'] = bedtime['date'].apply(to_doy)\n",
    "    bedtime['day'] = bedtime['day'].dt.dayofyear\n",
    "\n",
    "    # study events include all studying that occured for over 20 minutes, calculated by finding the times when \n",
    "    # a student was in a study location (ie libraries) from the wifi location data \n",
    "    study_events = pd.read_csv('tables/study_events/study_events.csv')\n",
    "    study_events['day'] = study_events['date'].apply(lambda x: dt.strptime(x, '%m/%d/%Y'))\n",
    "    study_events['day'] = study_events['day'].dt.dayofyear\n",
    "    study_events = study_events.rename(columns = {'event_delta': 'study duration'})\n",
    "\n",
    "    # study quietness cross references audio with study events\n",
    "    study_quietness = pd.read_csv('tables/study_quiteness/study_quiteness.csv')\n",
    "    study_quietness['day'] = pd.to_datetime(study_quietness['timestamp'], unit = 's').dt.dayofyear\n",
    "    study_quietness = study_quietness.rename(columns = {' audio inference': 'noise while studying'})\n",
    "\n",
    "    # study stillness cross references activity with study events\n",
    "    study_stillness = pd.read_csv('tables/study_stillness/study_stillness.csv')\n",
    "    study_stillness['day'] = pd.to_datetime(study_stillness['timestamp'], unit = 's').dt.dayofyear\n",
    "    study_stillness = study_stillness.rename(columns = {' activity inference': 'activity while studying'})\n",
    "\n",
    "    # darkness data \n",
    "    darkness = pd.read_csv('tables/dark/dark.csv')\n",
    "    darkness = darkness.rename(columns = {'duration': 'darkness duration'})\n",
    "    darkness['day'] = darkness['date'].apply(lambda x: dt.strptime(x, '%m/%d/%Y')).dt.dayofyear\n",
    "    \n",
    "    return bedtime, study_events, study_quietness, study_stillness, darkness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedtime, study_events, study_quietness, study_stillness, darkness = process_sensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_partying():\n",
    "    \"\"\"\n",
    "    I found the partying inference data in another notebook, called analyze audio, partying, and saved it to a csv. \n",
    "    Here I load the csv and make a day column for the partying feature. \n",
    "    \"\"\"\n",
    "    partying = pd.read_csv('dataset/sensing/partying/partying.csv')\n",
    "    partying['day'] = pd.to_datetime(partying['start time'], unit='s').dt.dayofyear\n",
    "    partying['day of week'] = pd.to_datetime(partying['start time'], unit = 's').dt.dayofweek\n",
    "    partying = partying.rename(columns = {'duration' : 'party duration'})\n",
    "    return partying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start time</th>\n",
       "      <th>end time</th>\n",
       "      <th>proportion loud labels</th>\n",
       "      <th>uid</th>\n",
       "      <th>party duration</th>\n",
       "      <th>day</th>\n",
       "      <th>day of week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.364923e+09</td>\n",
       "      <td>1.364927e+09</td>\n",
       "      <td>0.938559</td>\n",
       "      <td>u45</td>\n",
       "      <td>4610.0</td>\n",
       "      <td>92</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.365018e+09</td>\n",
       "      <td>1.365021e+09</td>\n",
       "      <td>0.903483</td>\n",
       "      <td>u45</td>\n",
       "      <td>2296.0</td>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.365021e+09</td>\n",
       "      <td>1.365023e+09</td>\n",
       "      <td>0.913175</td>\n",
       "      <td>u45</td>\n",
       "      <td>1902.0</td>\n",
       "      <td>93</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.365189e+09</td>\n",
       "      <td>1.365193e+09</td>\n",
       "      <td>0.965458</td>\n",
       "      <td>u45</td>\n",
       "      <td>4070.0</td>\n",
       "      <td>95</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.365257e+09</td>\n",
       "      <td>1.365259e+09</td>\n",
       "      <td>0.713089</td>\n",
       "      <td>u45</td>\n",
       "      <td>1833.0</td>\n",
       "      <td>96</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>1.367823e+09</td>\n",
       "      <td>1.367829e+09</td>\n",
       "      <td>0.731011</td>\n",
       "      <td>u31</td>\n",
       "      <td>5746.0</td>\n",
       "      <td>126</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>1.367935e+09</td>\n",
       "      <td>1.367939e+09</td>\n",
       "      <td>0.735425</td>\n",
       "      <td>u31</td>\n",
       "      <td>3297.0</td>\n",
       "      <td>127</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4050</th>\n",
       "      <td>1.368211e+09</td>\n",
       "      <td>1.368216e+09</td>\n",
       "      <td>0.941014</td>\n",
       "      <td>u31</td>\n",
       "      <td>4225.0</td>\n",
       "      <td>130</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>1.368367e+09</td>\n",
       "      <td>1.368372e+09</td>\n",
       "      <td>0.942281</td>\n",
       "      <td>u31</td>\n",
       "      <td>5348.0</td>\n",
       "      <td>132</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>1.369322e+09</td>\n",
       "      <td>1.369324e+09</td>\n",
       "      <td>0.724008</td>\n",
       "      <td>u31</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>143</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4053 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        start time      end time  proportion loud labels  uid  party duration  \\\n",
       "0     1.364923e+09  1.364927e+09                0.938559  u45          4610.0   \n",
       "1     1.365018e+09  1.365021e+09                0.903483  u45          2296.0   \n",
       "2     1.365021e+09  1.365023e+09                0.913175  u45          1902.0   \n",
       "3     1.365189e+09  1.365193e+09                0.965458  u45          4070.0   \n",
       "4     1.365257e+09  1.365259e+09                0.713089  u45          1833.0   \n",
       "...            ...           ...                     ...  ...             ...   \n",
       "4048  1.367823e+09  1.367829e+09                0.731011  u31          5746.0   \n",
       "4049  1.367935e+09  1.367939e+09                0.735425  u31          3297.0   \n",
       "4050  1.368211e+09  1.368216e+09                0.941014  u31          4225.0   \n",
       "4051  1.368367e+09  1.368372e+09                0.942281  u31          5348.0   \n",
       "4052  1.369322e+09  1.369324e+09                0.724008  u31          2211.0   \n",
       "\n",
       "      day  day of week  \n",
       "0      92            1  \n",
       "1      93            2  \n",
       "2      93            2  \n",
       "3      95            4  \n",
       "4      96            5  \n",
       "...   ...          ...  \n",
       "4048  126            0  \n",
       "4049  127            1  \n",
       "4050  130            4  \n",
       "4051  132            6  \n",
       "4052  143            3  \n",
       "\n",
       "[4053 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partying = load_partying()\n",
    "partying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_activities(): \n",
    "    # since all the activity data takes a very long time to load, i'll continue processing it in this new function \n",
    "    # alone so i can debug more easily\n",
    "    return pd.read_csv('tables/activity/activity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_epochs(activity):\n",
    "    # these lines find the day and hour of activity inference\n",
    "    activity['day'] = activity['date'].apply(lambda x: dt.strptime(x, '%Y-%m-%d')).dt.dayofyear\n",
    "\n",
    "    activity['epoch'] = pd.to_datetime(activity['time']).dt.hour\n",
    "\n",
    "    # next, apply the epoch function to find the epoch of each activity\n",
    "    activity['epoch'] = list(zip(activity['epoch'], activity['epoch']))\n",
    "\n",
    "    activity['epoch'] = activity['epoch'].apply(epoch)\n",
    "    \n",
    "    return activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_activities(activity): \n",
    "    # This code is a little clunky, but I thought formatting it in a function would take a longer runtime because it would \n",
    "    # require more for loops, and the activity data already takes a long time to process since it is a large dataset. \n",
    "\n",
    "    # the goal of this cell is to create a new dataframe with the average activity inference for each day for each user. \n",
    "    # it will also make dataframes \n",
    "\n",
    "    # I will aggregate the data in the folloing data frames\n",
    "    aggreg = pd.DataFrame()\n",
    "    day_aggreg = pd.DataFrame()\n",
    "    evening_aggreg = pd.DataFrame()\n",
    "    night_aggreg = pd.DataFrame()\n",
    "\n",
    "    # loop through each uid\n",
    "    for uid in activity.uid.unique(): \n",
    "\n",
    "        # take the uid specific data, group it by day, find the mean, and then append each dataframe to its respective aggregate.\n",
    "        uid_data = activity[activity['uid'] == uid]\n",
    "        day = uid_data[uid_data['epoch'] == 'Day']\n",
    "        evening = uid_data[uid_data['epoch'] == 'Evening']\n",
    "        night = uid_data[uid_data['epoch'] == 'Night']\n",
    "\n",
    "        uid_data = uid_data.groupby('day').mean()\n",
    "        uid_data['uid'] = uid\n",
    "\n",
    "        day = day.groupby('day').mean()\n",
    "        day['uid'] = uid\n",
    "\n",
    "        evening = evening.groupby('day').mean()\n",
    "        evening['uid'] = uid\n",
    "\n",
    "        night = night.groupby('day').mean()  \n",
    "        night['uid'] = uid\n",
    "\n",
    "        aggreg = aggreg.append(uid_data)\n",
    "        day_aggreg = day_aggreg.append(day)\n",
    "        evening_aggreg = evening_aggreg.append(evening)\n",
    "        night_aggreg = night_aggreg.append(night)\n",
    "        \n",
    "    # make the columns in each dataframe more descriptive\n",
    "    aggreg['day'] = aggreg.index\n",
    "    activity = aggreg\n",
    "    \n",
    "    # make day activities dataframe\n",
    "    day_aggreg['day'] = day_aggreg.index\n",
    "    day_activity = day_aggreg\n",
    "    day_activity = day_activity.rename(columns = {' activity inference': 'day activity inference'})\n",
    "\n",
    "    # make evening activities dataframe\n",
    "    evening_aggreg['day'] = evening_aggreg.index\n",
    "    evening_activity = evening_aggreg\n",
    "    evening_activity = evening_activity.rename(columns = {' activity inference': 'evening activity inference'})\n",
    "\n",
    "    # make night activities dataframe\n",
    "    night_aggreg['day'] = night_aggreg.index\n",
    "    night_activity = night_aggreg\n",
    "    night_activity = night_activity.rename(columns ={' activity inference': 'night activity inference'})\n",
    "    \n",
    "    return activity, day_activity, evening_activity, night_activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_conversations(): \n",
    "    # processing conversation data, adding day and hour columns. \n",
    "    conversation = pd.read_csv('tables/conversation/conversation.csv')\n",
    "    conversation['day'] = conversation['date'].apply(to_doy).dt.dayofyear\n",
    "    conversation['start hour'] = pd.to_datetime(conversation['start']).dt.hour\n",
    "    conversation['end hour'] = pd.to_datetime(conversation['end']).dt.hour\n",
    "\n",
    "    # this allows us to use apply to add the epoch column\n",
    "    conversation['epoch'] = list(zip(conversation['start hour'], conversation['end hour']))\n",
    "\n",
    "    # apply the epoch function to create a new column which contains the epoch for every conversation\n",
    "    conversation['epoch'] = conversation['epoch'].apply(epoch)\n",
    "\n",
    "    # make new dataframes for each epoch\n",
    "    conversations_day = conversation[conversation['epoch'] == 'Day']\n",
    "    conversations_day = conversations_day.rename(columns = {'duration': 'day convo duration'})\n",
    "\n",
    "    conversations_evening = conversation[conversation['epoch'] == 'Evening']\n",
    "    conversations_evening = conversations_evening.rename(columns = {'duration': 'evening convo duration'})\n",
    "\n",
    "    conversations_night = conversation[conversation['epoch'] == 'Night']\n",
    "    conversations_night = conversations_night.rename(columns = {'duration': 'night convo duration'})\n",
    "\n",
    "    conversation = conversation.rename(columns = {'duration': 'total convo duration'})\n",
    "    \n",
    "    return conversation, conversations_day, conversations_evening, conversations_night"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities = load_activities()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "activity, day_activity, evening_activity, night_activity = aggregate_activities(activity_epochs(activities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation, conversations_day, conversations_evening, conversations_night = process_conversations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades = pd.read_csv('tables/grades/grades.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_indoor_mobility():\n",
    "    \"\"\"\n",
    "    I processed the indoor mobility in another notebook called \"indoor mobility\" and saved it on my computer. The \n",
    "    basic method was matching up time intervals between indoor location data and activity inference data then finding \n",
    "    the average indoor activity inference per day. \n",
    "    \"\"\"\n",
    "    indoor_mob = pd.read_csv('dataset/sensing/indoor_mobility/indoor_mobility.csv')\n",
    "    day_im = pd.read_csv('dataset/sensing/indoor_mobility/day_indoor_mobility.csv')\n",
    "    evening_im = pd.read_csv('dataset/sensing/indoor_mobility/evening_indoor_mobility.csv')\n",
    "    night_im = pd.read_csv('dataset/sensing/indoor_mobility/night_indoor_mobility.csv')\n",
    "    return indoor_mob, day_im, evening_im, night_im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "indoor_mob, day_im, evening_im, night_im = load_indoor_mobility()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this aggregates data from all the EMA's already processed in the tables folder. These include Activty, Exercise, \n",
    "# mood, sleep, social and stress, which seem pretty descriptive for mental health. \n",
    "def compile_emas(): \n",
    "    ema_files = glob.glob('tables/EMA/*/*.csv')\n",
    "    emas = pd.DataFrame()\n",
    "    for file in ema_files: \n",
    "        current = pd.read_csv(file)\n",
    "        emas = pd.concat([emas, current])\n",
    "\n",
    "    # each uid will only have one column\n",
    "    emas = emas.groupby('uid').sum()\n",
    "    return emas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(49, 17)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emas = compile_emas()\n",
    "emas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_slopes_df(df, uid, x_column, y_column, slopes_df): \n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        df: dataframe containing x_column, y_column, and a day column representing the day of the year\n",
    "        uid: desired student id\n",
    "        slopes_df: dataframe we want to append results to\n",
    "    This function calculates slopes for y vs. x over the whole term, before the midterm, and after the midterm, and\n",
    "    also the means of y for those periods. \n",
    "    outputs: appends slopes and means for the whole term, before breakpoint, and after breakpoint to the slopes_df\n",
    "             dataframe and returns that dataframe. \n",
    "    \n",
    "    The logic behind this is that maybe some time periods are more descriptive of a student's mental picture than others. \n",
    "    Maybe the student had great sleep before the midterm, but really fell off after the midterm, and got bad grades \n",
    "    because of it. This may not be represented in the overall mean, but will be easy to see in that student's post \n",
    "    breakpoint slope and mean. \n",
    "    \"\"\"\n",
    "    \n",
    "    # look at only the chosen uid. \n",
    "    uid_data = df[df.uid == uid]\n",
    "    slope = linear_coef(uid_data[x_column], uid_data[y_column])\n",
    "    mean = uid_data[y_column].mean()\n",
    "    \n",
    "    # find the breakpoint\n",
    "    breakpoint = behavioral_breakpoint(uid_data, x_column, y_column)\n",
    "    \n",
    "    pre_breakpoint_df = uid_data[uid_data['day'] < breakpoint]\n",
    "    \n",
    "    try: \n",
    "        # finds slope and mean before the breakpoint\n",
    "        pre_breakpoint_slope = linear_coef(pre_breakpoint_df[x_column], pre_breakpoint_df[y_column])\n",
    "        pre_breakpoint_mean = pre_breakpoint_df[y_column].mean()\n",
    "    except: \n",
    "        # if there's no data for this student, their mean is 0 and slope is 0\n",
    "        pre_breakpoint_slope = 0\n",
    "        pre_breakpoint_mean = 0\n",
    "    \n",
    "    post_breakpoint_df = uid_data[uid_data['day'] > breakpoint]\n",
    "    \n",
    "    try: \n",
    "        # finds slope and mean after the breakpoint\n",
    "        post_breakpoint_slope = linear_coef(post_breakpoint_df[x_column], post_breakpoint_df[y_column])\n",
    "        post_breakpoint_mean = post_breakpoint_df[y_column].mean()\n",
    "    except: \n",
    "        # again handling the case of insufficient data\n",
    "        post_breakpoint_mean = 0\n",
    "        post_breakpoint_slope = 0\n",
    "\n",
    "    # append all the new calculations to the dataframe\n",
    "    slopes_df = slopes_df.append(\n",
    "        pd.DataFrame({'uid': uid, y_column +' slope' : slope, y_column + ' mean': mean, \n",
    "                      y_column + ' breakpoint': breakpoint, \n",
    "                      y_column + ' pre slope': pre_breakpoint_slope, y_column + ' pre mean': pre_breakpoint_mean,\n",
    "                      y_column + ' post slope': post_breakpoint_slope, y_column + ' post mean': post_breakpoint_mean,\n",
    "                                              }), ignore_index = True)\n",
    "    \n",
    "    return slopes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_emas(ema_name): \n",
    "    \"\"\"\n",
    "    input: ema_name -- name of the ema we're looking at\n",
    "    output: dataframe containing the data from the desired ema compiled for all uids\n",
    "    \"\"\"\n",
    "    # list of all the desired ema's \n",
    "    all_emas = glob.glob('dataset/EMA/response/' + ema_name + '/' + ema_name +'_*.json')\n",
    "    # index to start the uid\n",
    "    uid_start = len('dataset/EMA/response/' + ema_name + '/' + ema_name +'_')\n",
    "    # this is where we'll compile the data\n",
    "    total_ema_data = pd.DataFrame()\n",
    "    # loops through all the ema data\n",
    "    for ema in all_emas: \n",
    "        # the uid is the three characters starting at uid_start index\n",
    "        uid = ema[uid_start:uid_start + 3]\n",
    "        # read the data\n",
    "        ema_data = pd.read_json(ema)\n",
    "        # keep track of uids\n",
    "        ema_data['uid'] = uid\n",
    "        # compile the data\n",
    "        total_ema_data = total_ema_data.append(ema_data)\n",
    "    \n",
    "    total_ema_data['day of week'] = total_ema_data['resp_time'].dt.dayofweek\n",
    "    total_ema_data['day'] = total_ema_data['resp_time'].dt.dayofyear\n",
    "    return total_ema_data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ema_post_slope(ema_df, col):\n",
    "    \"\"\"\n",
    "    input: dataframe with ema data, col which is the desired column for the slope\n",
    "    output: slope for the desired column after the breakpoint for the term for each user id \n",
    "    \"\"\"\n",
    "    agg_df = pd.DataFrame()\n",
    "    \n",
    "    for uid in ema_df['uid'].unique(): \n",
    "        \n",
    "        # take the data specific to each user and average them by day. \n",
    "        uid_ema = ema_df[ema_df.uid == uid]\n",
    "        uid_ema = uid_ema.groupby('day').mean()\n",
    "        \n",
    "        # the index is going to be day since we grouped by day\n",
    "        uid_ema['day'] = uid_ema.index\n",
    "        \n",
    "        # this function finds the behavioral breakpoint of the ema data. See the function for further explanation\n",
    "        breakpoint = behavioral_breakpoint(uid_ema, 'day', col)\n",
    "        \n",
    "        # only take data after the breakpoint\n",
    "        post_breakpoint_data = uid_ema[uid_ema['day'] > breakpoint]\n",
    "        \n",
    "        \n",
    "        slope = linear_coef(post_breakpoint_data['day'], post_breakpoint_data[col])\n",
    "        \n",
    "        agg_df = agg_df.append(pd.DataFrame({'uid': uid, col + ' slope': slope}))\n",
    "    \n",
    "    return agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def behavioral_breakpoint(data, x_column, y_column, plot = False): \n",
    "    \"\"\"\n",
    "    inputs: \n",
    "        data: dataframe to look at\n",
    "        x_column: independent variable in data\n",
    "        y_column: dependent variable in data\n",
    "    outputs: \n",
    "        the x_column value where the slope for y_column changes the most during the course of x_column, ie the \n",
    "        behavioral breakpoint\n",
    "        \n",
    "    to calculate the breakpoint, the ruptures package splits the data into two groups so that \n",
    "    it minimizes the l1 cost function of the lines of best fit through each group. l1 cost function is the \n",
    "    sum of the absolute value of the differences between the estimated regression values and the actual values.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        feat = data[[x_column, y_column]].values\n",
    "\n",
    "        # ruptures algorithm for finding 1 breakpoint in the data\n",
    "        algo = rpt.Dynp(model = 'l1', min_size = 2).fit(feat)\n",
    "        my_bkps = algo.predict(n_bkps = 1)\n",
    "\n",
    "        if plot is True:\n",
    "            # ruptures automatically plots the breakpoints and loss function with this function\n",
    "            rpt.show.display(feat, my_bkps, figsize=(10, 6))\n",
    "            plt.xlabel(x_column + ' index')\n",
    "            plt.ylabel(y_column)\n",
    "            plt.show()\n",
    "            \n",
    "        # this finds the day where the breakpoint occurs (it's initially given as an index)\n",
    "        x_value_bkp = feat[my_bkps[0], 0]\n",
    "        \n",
    "    except: \n",
    "        # if no eligible breakpoint can be found, assume it is the midterm. \n",
    "        # This occurs in the case of not enough datapoints. \n",
    "        x_value_bkp = 35\n",
    "    \n",
    "    return x_value_bkp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "112.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of using the behavioral breakpoint function with plotting. \n",
    "behavioral_breakpoint(bedtime[bedtime.uid == 'u05'], 'day', 'night_duration' ,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seating_row(seat_number): \n",
    "    \"\"\"\n",
    "    input: seat number\n",
    "    output: section of that seat number, none if the input isn't a valid seat number.\n",
    "    \"\"\"\n",
    "    try: \n",
    "        seat_number = int(seat_number)\n",
    "        for i in range(0, 6): \n",
    "            if seat_number in range(i*14 + 1, (i+1)*14 + 1): \n",
    "                return i + 1\n",
    "        return np.NaN\n",
    "    except: \n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def usual_row(file): \n",
    "    \"\"\"\n",
    "    input: a json file of QR codes \n",
    "    output: the most sat-in row for that student. \n",
    "    \"\"\"\n",
    "    try: \n",
    "        seats = pd.read_json(file)\n",
    "        seats['qr_code'] = seats['qr_code'].apply(seating_row)\n",
    "        return seats['qr_code'].mode().values[0]\n",
    "    except: \n",
    "        return \"no qr results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_seating():\n",
    "    # this loops through the seating files and adds them to a single dataframe\n",
    "    seating_files = glob.glob(\"dataset/EMA/response/QR_code/QR_*.json\")\n",
    "    uid_start = len(\"dataset/EMA/response/QR_code/QR_\")\n",
    "\n",
    "    uid_list = []\n",
    "    most_frequent_row = [] \n",
    "\n",
    "    for file in seating_files: \n",
    "        uid = file[uid_start:uid_start+3]\n",
    "        uid_list.append(uid)\n",
    "\n",
    "        most_frequent_row.append(usual_row(file))\n",
    "\n",
    "    seating_df = pd.DataFrame({'uid': uid_list, 'row': most_frequent_row})\n",
    "    \n",
    "    return seating_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seating_df = apply_seating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_five_nums(response): \n",
    "    \"\"\"\n",
    "    given a big five response, converts that response into a number corresponding to it's significance level\n",
    "    1 - disagree strongly to 5 - agree strongly\n",
    "    \"\"\"\n",
    "    if response == 'Disagree Strongly': \n",
    "        return 1\n",
    "    elif response == 'Disagree a little': \n",
    "        return 2\n",
    "    elif response == 'Neither agree nor disagree': \n",
    "        return 3\n",
    "    elif response == 'Agree a little':\n",
    "        return 4\n",
    "    elif response == 'Agree strongly': \n",
    "        return 5\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def big_five_c_processing(): \n",
    "    \"\"\"\n",
    "    returns a dataframe containing conscientiousness scores for each user\n",
    "    \"\"\"\n",
    "    big_five = pd.read_csv('dataset/survey/bigfive.csv')\n",
    "    big_five = big_five.applymap(big_five_nums)\n",
    "    # indices match the survey given here:\n",
    "    # https://uasdata.usc.edu/index.php?r=eNpLtDKyqi62MrFSKkhMT1WyLrYyNAeyS5NyMpP1UhJLEvUSU1Ly80ASQDWJKZkpYDVWSsZK1rVcMEfoEnY    positive_C = big_five.iloc[:, [2, ]]\n",
    "    positive_C = big_five.iloc[:, [0, 1, 4, 14, 29, 34, 39]]\n",
    "    negative_C = big_five.iloc[:, [9, 19, 24, 44]]\n",
    "    reversed_negs = negative_C.applymap(lambda x: 6 - x)\n",
    "    \n",
    "    total = pd.concat([positive_C, negative_C], axis = 1)\n",
    "    total['total conscientiousness'] = total.sum(axis = 1)\n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "useful_c = big_five_c_processing()[['uid', 'total conscientiousness']][big_five_c_processing()['type'] == 'pre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attendance(): \n",
    "    \"\"\"\n",
    "    the class attendance data is loaded from a jupyter notebook made by omkar.\n",
    "    \"\"\"\n",
    "    class_attendance = pd.read_csv('dataset/education/Class Attendance.csv')\n",
    "    att_per_student = class_attendance.groupby('uid').mean()\n",
    "    att_per_student['id'] = att_per_student.index\n",
    "    \n",
    "    att_per_student.hist(column = 'Actual Rates')\n",
    "    plt.title('Histogram of Attendance rates per student')\n",
    "    plt.xlabel('Attendance Rate')\n",
    "    plt.ylabel('Number of Students')\n",
    "    \n",
    "    return att_per_student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_attendance = load_attendance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_by_dow = class_attendance.groupby('Weekday').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_dataframe(): \n",
    "    # this dataframe will contain all the features to learn on\n",
    "    behavioral_df = pd.DataFrame()\n",
    "\n",
    "    # this list contains all tuples of (dataframe, column) that I want to add to the features dataframe. \n",
    "    to_slope = [(bedtime, 'night_duration'), \n",
    "                (study_quietness, 'noise while studying'), \n",
    "                (study_stillness, 'activity while studying'),\n",
    "                (study_events, 'study duration'), \n",
    "                (darkness, 'darkness duration'),\n",
    "                (conversations_day, 'day convo duration'), \n",
    "                (conversations_evening, 'evening convo duration'),\n",
    "                (conversations_night, 'night convo duration'),\n",
    "                (conversation, 'total convo duration'), \n",
    "                (activity, ' activity inference'),\n",
    "                (day_activity, 'day activity inference'), \n",
    "                (evening_activity, 'evening activity inference'),\n",
    "                (night_activity, 'night activity inference'), \n",
    "                (indoor_mob, 'average indoor mobility'), \n",
    "                (day_im, 'day indoor mobility'), \n",
    "                (evening_im, 'evening indoor mobility'), \n",
    "                (night_im, 'night indoor mobility'), \n",
    "                (partying, 'party duration')\n",
    "                ]\n",
    "\n",
    "    # will save runtime by only looking at uids in grades since those are the only ones of interest. \n",
    "    grades = pd.read_csv('tables/grades/grades.csv')\n",
    "\n",
    "    # loop through all uids\n",
    "    for uid in grades['uid'].unique(): \n",
    "        # loop through each (dataframe, column) tuple\n",
    "        for tup in to_slope: \n",
    "            # the uid must be in the dataframe to process it\n",
    "            if uid in tup[0].uid.unique(): \n",
    "                # add slopes, means and breakpoints to the behavioral_df using the append_slopes function \n",
    "                df = tup[0]\n",
    "                column = tup[1]\n",
    "                try: \n",
    "                    behavioral_df = append_slopes_df(df, uid, 'day', column, behavioral_df)\n",
    "                except: \n",
    "                    behavioral_df = append_slopes_df(df.dropna(), uid, 'day', column, behavioral_df)\n",
    "\n",
    "    # this dataframe \n",
    "    behavioral_df = behavioral_df.groupby('uid').sum()\n",
    "    \n",
    "    # add data from gps paper, including location variance, speed mean, homestay, and others. \n",
    "    gps_transforms = pd.read_csv(\"GPS_paper/features_10week_all.csv\")\n",
    "\n",
    "    # add gps transforms to the behavioral dataframe\n",
    "    behavioral_df = behavioral_df.merge(gps_transforms, left_on = 'uid', right_on = 'id', how = 'inner')\n",
    "\n",
    "    # merge with Ema data\n",
    "    behavioral_df = behavioral_df.merge(emas, left_on = 'id', right_on = 'uid', how = 'inner')\n",
    "\n",
    "    # add seating data to the behavioral dataframe\n",
    "    behavioral_df = behavioral_df.merge(seating_df, left_on = 'id', right_on = 'uid', how = 'inner')\n",
    "    \n",
    "    # add conscientiousness data\n",
    "    behavioral_df = behavioral_df.merge(useful_c, left_on = 'id', right_on = 'uid', how = 'left')\n",
    "    behavioral_df['total conscientiousness'] = behavioral_df['total conscientiousness'].fillna(\n",
    "                                                behavioral_df['total conscientiousness'].mean())\n",
    "    \n",
    "    behavioral_df = behavioral_df.merge(class_attendance[['id', 'Actual Rates']], on = 'id', how = 'left')\n",
    "    behavioral_df['Actual Rates'] = behavioral_df['Actual Rates'].fillna(0)\n",
    "    behavioral_df = behavioral_df.rename(columns = {'Actual Rates': 'Class Attendance'})\n",
    "    \n",
    "    # drop id columns so all the dataframe is left with is features (they are ordered, so still correspond with the ordered\n",
    "    # GPA uids)\n",
    "    behavioral_df = behavioral_df.drop(columns = ['uid_x', 'id', 'uid_y'])\n",
    "    \n",
    "    # this function just adds some extra features. \n",
    "    feat_df = extract_7_features_df(behavioral_df)\n",
    "    \n",
    "    print(behavioral_df.shape)\n",
    "    print(feat_df.shape)\n",
    "    \n",
    "    behavioral_df = pd.concat([behavioral_df, feat_df[['positive affect', 'stress', 'positive affect post slope']]], axis=1)\n",
    "\n",
    "    return behavioral_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_7_features_df(behavioral_df): \n",
    "    \"\"\"\n",
    "    creates a dataframe of the 7 features selected by the LASSO regression from the smartgpa paper. these features a\n",
    "    are conversation duration during night breakpoint, positive affect, conversation duration during evening slope, \n",
    "    study duration, positive affect post-slope, stress term-slope, and conscientiousness.\n",
    "    \"\"\"\n",
    "    \n",
    "    PAM = combine_emas('PAM')\n",
    "    PAM = PAM[PAM['uid'].isin(grades.uid)]\n",
    "    \n",
    "    PAM_mean = PAM.groupby('uid')['picture_idx'].mean()\n",
    "    \n",
    "    PAM_slopes = ema_post_slope(PAM, 'picture_idx')\n",
    "    \n",
    "    imp_study_events = study_events[study_events['uid'].isin(grades.uid)]\n",
    "    study_duration = imp_study_events.groupby('uid')['study duration'].sum()\n",
    "    \n",
    "    conscientiousness = useful_c[useful_c['uid'].isin(grades.uid)]\n",
    "    \n",
    "    stress = combine_emas('Stress')\n",
    "    stress = stress[stress.uid.isin(grades.uid)]\n",
    "    stress_mean = stress.groupby('uid')['level'].mean()\n",
    "    \n",
    "    \n",
    "    small_features_df = pd.DataFrame({'positive affect': PAM_mean.values, \n",
    "                                      'night convo duration breakpoint': behavioral_df['night convo duration breakpoint'], \n",
    "                                      'evening convo duration term slope': behavioral_df['evening convo duration slope'], \n",
    "                                      'study duration': study_duration.values, \n",
    "                                      'conscientiousness': behavioral_df['total conscientiousness'], \n",
    "                                      'positive affect post slope': PAM_slopes['picture_idx slope'].values,\n",
    "                                      'stress': stress_mean.values\n",
    "                                     })\n",
    "    \n",
    "    return small_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30, 157)\n",
      "(30, 7)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(30, 160)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavioral_df = create_feature_dataframe()\n",
    "behavioral_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>night_duration slope</th>\n",
       "      <th>night_duration mean</th>\n",
       "      <th>night_duration breakpoint</th>\n",
       "      <th>night_duration pre slope</th>\n",
       "      <th>night_duration pre mean</th>\n",
       "      <th>night_duration post slope</th>\n",
       "      <th>night_duration post mean</th>\n",
       "      <th>noise while studying slope</th>\n",
       "      <th>noise while studying mean</th>\n",
       "      <th>noise while studying breakpoint</th>\n",
       "      <th>...</th>\n",
       "      <th>sleep_quality</th>\n",
       "      <th>sleep_rested</th>\n",
       "      <th>people_interacted_with</th>\n",
       "      <th>stress_level</th>\n",
       "      <th>row</th>\n",
       "      <th>total conscientiousness</th>\n",
       "      <th>Class Attendance</th>\n",
       "      <th>positive affect</th>\n",
       "      <th>stress</th>\n",
       "      <th>positive affect post slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.164093</td>\n",
       "      <td>421.995238</td>\n",
       "      <td>119.0</td>\n",
       "      <td>4.020310</td>\n",
       "      <td>431.862500</td>\n",
       "      <td>-1.448899</td>\n",
       "      <td>400.164286</td>\n",
       "      <td>-0.012393</td>\n",
       "      <td>0.963071</td>\n",
       "      <td>122.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.350000</td>\n",
       "      <td>1.850000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>4</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.607843</td>\n",
       "      <td>9.971429</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.085333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-2.059699</td>\n",
       "      <td>428.346296</td>\n",
       "      <td>115.0</td>\n",
       "      <td>-1.166852</td>\n",
       "      <td>468.026667</td>\n",
       "      <td>-1.709380</td>\n",
       "      <td>393.633333</td>\n",
       "      <td>-0.004352</td>\n",
       "      <td>1.114056</td>\n",
       "      <td>131.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.052632</td>\n",
       "      <td>1.526316</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>3</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>2.066667</td>\n",
       "      <td>-0.022219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.617033</td>\n",
       "      <td>417.591176</td>\n",
       "      <td>104.0</td>\n",
       "      <td>7.012367</td>\n",
       "      <td>471.046667</td>\n",
       "      <td>0.099656</td>\n",
       "      <td>393.591429</td>\n",
       "      <td>0.006630</td>\n",
       "      <td>1.269486</td>\n",
       "      <td>117.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.074074</td>\n",
       "      <td>2.232558</td>\n",
       "      <td>2</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.651376</td>\n",
       "      <td>1.860465</td>\n",
       "      <td>0.058258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.324367</td>\n",
       "      <td>451.678205</td>\n",
       "      <td>112.0</td>\n",
       "      <td>-4.400206</td>\n",
       "      <td>465.770000</td>\n",
       "      <td>-1.679831</td>\n",
       "      <td>435.131481</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>1.452299</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>3.875000</td>\n",
       "      <td>4</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.622807</td>\n",
       "      <td>9.333333</td>\n",
       "      <td>3.375000</td>\n",
       "      <td>0.700450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.220315</td>\n",
       "      <td>452.329167</td>\n",
       "      <td>112.0</td>\n",
       "      <td>-1.304891</td>\n",
       "      <td>413.399306</td>\n",
       "      <td>-1.210151</td>\n",
       "      <td>532.116667</td>\n",
       "      <td>-0.082633</td>\n",
       "      <td>0.578431</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.066667</td>\n",
       "      <td>1.066667</td>\n",
       "      <td>3.250000</td>\n",
       "      <td>3.773585</td>\n",
       "      <td>1</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.993197</td>\n",
       "      <td>3.245283</td>\n",
       "      <td>0.310667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.591977</td>\n",
       "      <td>350.504645</td>\n",
       "      <td>108.0</td>\n",
       "      <td>-4.377251</td>\n",
       "      <td>366.001333</td>\n",
       "      <td>1.041262</td>\n",
       "      <td>341.203810</td>\n",
       "      <td>0.007990</td>\n",
       "      <td>1.200926</td>\n",
       "      <td>114.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.181818</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>2.767442</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>6</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>8.863636</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.058047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.252891</td>\n",
       "      <td>502.914286</td>\n",
       "      <td>123.0</td>\n",
       "      <td>-1.101175</td>\n",
       "      <td>488.478333</td>\n",
       "      <td>-1.454870</td>\n",
       "      <td>512.903333</td>\n",
       "      <td>0.006853</td>\n",
       "      <td>1.057366</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>6</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.171226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>-1.081539</td>\n",
       "      <td>427.941481</td>\n",
       "      <td>116.0</td>\n",
       "      <td>1.226258</td>\n",
       "      <td>447.687222</td>\n",
       "      <td>-1.082490</td>\n",
       "      <td>392.161538</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.898692</td>\n",
       "      <td>123.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.820513</td>\n",
       "      <td>1.230769</td>\n",
       "      <td>3.133333</td>\n",
       "      <td>3.320755</td>\n",
       "      <td>1</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>9.706070</td>\n",
       "      <td>2.396226</td>\n",
       "      <td>-0.017630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.148476</td>\n",
       "      <td>467.779667</td>\n",
       "      <td>133.0</td>\n",
       "      <td>0.167124</td>\n",
       "      <td>485.322381</td>\n",
       "      <td>-3.901980</td>\n",
       "      <td>422.122619</td>\n",
       "      <td>0.004033</td>\n",
       "      <td>0.941023</td>\n",
       "      <td>146.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>1.150000</td>\n",
       "      <td>2.055556</td>\n",
       "      <td>2.766667</td>\n",
       "      <td>1</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.965174</td>\n",
       "      <td>2.233333</td>\n",
       "      <td>0.090601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.519706</td>\n",
       "      <td>406.429861</td>\n",
       "      <td>106.0</td>\n",
       "      <td>1.850716</td>\n",
       "      <td>420.232639</td>\n",
       "      <td>1.574997</td>\n",
       "      <td>386.098485</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>1.432985</td>\n",
       "      <td>116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.440000</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>3.230769</td>\n",
       "      <td>2.054054</td>\n",
       "      <td>1</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>8.176768</td>\n",
       "      <td>2.162162</td>\n",
       "      <td>0.016784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-2.421907</td>\n",
       "      <td>458.003333</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.421907</td>\n",
       "      <td>458.003333</td>\n",
       "      <td>0.022534</td>\n",
       "      <td>0.970043</td>\n",
       "      <td>135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>5</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>9.075472</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.114005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.707225</td>\n",
       "      <td>452.577778</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.129302</td>\n",
       "      <td>461.690000</td>\n",
       "      <td>-1.755000</td>\n",
       "      <td>419.206667</td>\n",
       "      <td>-0.075664</td>\n",
       "      <td>0.277277</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.035088</td>\n",
       "      <td>1.210526</td>\n",
       "      <td>3.025000</td>\n",
       "      <td>2.280374</td>\n",
       "      <td>1</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>9.359331</td>\n",
       "      <td>2.130841</td>\n",
       "      <td>-0.029593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.341172</td>\n",
       "      <td>372.851064</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1.908154</td>\n",
       "      <td>453.731667</td>\n",
       "      <td>0.329594</td>\n",
       "      <td>355.362963</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>1.398376</td>\n",
       "      <td>120.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.294118</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>2.451613</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>11.260465</td>\n",
       "      <td>2.857143</td>\n",
       "      <td>-0.019592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.726586</td>\n",
       "      <td>474.776190</td>\n",
       "      <td>105.0</td>\n",
       "      <td>-0.471886</td>\n",
       "      <td>494.968333</td>\n",
       "      <td>1.389038</td>\n",
       "      <td>444.551852</td>\n",
       "      <td>-0.022732</td>\n",
       "      <td>0.800024</td>\n",
       "      <td>135.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>1.769231</td>\n",
       "      <td>1.954545</td>\n",
       "      <td>2.625000</td>\n",
       "      <td>4</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.427083</td>\n",
       "      <td>11.182390</td>\n",
       "      <td>1.875000</td>\n",
       "      <td>0.026501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.341595</td>\n",
       "      <td>478.411111</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-4.244512</td>\n",
       "      <td>496.958889</td>\n",
       "      <td>0.798306</td>\n",
       "      <td>470.617544</td>\n",
       "      <td>0.017793</td>\n",
       "      <td>0.725473</td>\n",
       "      <td>143.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.346939</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>3.487179</td>\n",
       "      <td>2.527473</td>\n",
       "      <td>3</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>11.942708</td>\n",
       "      <td>1.978022</td>\n",
       "      <td>0.012697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-2.499837</td>\n",
       "      <td>412.676667</td>\n",
       "      <td>130.0</td>\n",
       "      <td>-3.308664</td>\n",
       "      <td>431.166111</td>\n",
       "      <td>8.013225</td>\n",
       "      <td>345.802083</td>\n",
       "      <td>0.014461</td>\n",
       "      <td>1.209935</td>\n",
       "      <td>115.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.615385</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.482759</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>5</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.629630</td>\n",
       "      <td>7.119048</td>\n",
       "      <td>2.490909</td>\n",
       "      <td>0.100732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-1.982719</td>\n",
       "      <td>437.020833</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.982719</td>\n",
       "      <td>437.020833</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.892072</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>1.187500</td>\n",
       "      <td>4.080000</td>\n",
       "      <td>2.512821</td>\n",
       "      <td>6</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>7.658683</td>\n",
       "      <td>1.948718</td>\n",
       "      <td>-0.029317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-6.311211</td>\n",
       "      <td>489.455556</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-6.311211</td>\n",
       "      <td>489.455556</td>\n",
       "      <td>-0.002431</td>\n",
       "      <td>1.237266</td>\n",
       "      <td>108.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>3.071429</td>\n",
       "      <td>3.150000</td>\n",
       "      <td>1</td>\n",
       "      <td>31.777778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>9.572917</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>-0.009404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.974266</td>\n",
       "      <td>444.091667</td>\n",
       "      <td>108.0</td>\n",
       "      <td>0.781878</td>\n",
       "      <td>468.135000</td>\n",
       "      <td>2.503125</td>\n",
       "      <td>417.600000</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>1.000449</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.416667</td>\n",
       "      <td>1.416667</td>\n",
       "      <td>2.125000</td>\n",
       "      <td>2.480000</td>\n",
       "      <td>5</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>9.434066</td>\n",
       "      <td>1.840000</td>\n",
       "      <td>0.018106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>-1.066047</td>\n",
       "      <td>466.454955</td>\n",
       "      <td>114.0</td>\n",
       "      <td>-0.879715</td>\n",
       "      <td>487.188333</td>\n",
       "      <td>0.794196</td>\n",
       "      <td>439.437500</td>\n",
       "      <td>0.003249</td>\n",
       "      <td>0.983238</td>\n",
       "      <td>88.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>1.107143</td>\n",
       "      <td>3.764706</td>\n",
       "      <td>3.533333</td>\n",
       "      <td>3</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.657407</td>\n",
       "      <td>9.850340</td>\n",
       "      <td>2.933333</td>\n",
       "      <td>-0.102190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>-0.763500</td>\n",
       "      <td>440.705556</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.552346</td>\n",
       "      <td>465.644444</td>\n",
       "      <td>0.225477</td>\n",
       "      <td>425.076087</td>\n",
       "      <td>-0.001783</td>\n",
       "      <td>1.099539</td>\n",
       "      <td>149.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.644444</td>\n",
       "      <td>1.311111</td>\n",
       "      <td>4.214286</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>6</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>0.621212</td>\n",
       "      <td>9.900000</td>\n",
       "      <td>2.469697</td>\n",
       "      <td>0.038915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>-1.142165</td>\n",
       "      <td>398.853546</td>\n",
       "      <td>104.0</td>\n",
       "      <td>1.223729</td>\n",
       "      <td>430.610000</td>\n",
       "      <td>1.573797</td>\n",
       "      <td>378.611290</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694444</td>\n",
       "      <td>1.472222</td>\n",
       "      <td>3.906250</td>\n",
       "      <td>2.586207</td>\n",
       "      <td>4</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.559524</td>\n",
       "      <td>8.304795</td>\n",
       "      <td>2.896552</td>\n",
       "      <td>-0.106482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.175806</td>\n",
       "      <td>545.935714</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175806</td>\n",
       "      <td>545.935714</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.747952</td>\n",
       "      <td>148.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.428571</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>31.777778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.390244</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.006684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.595732</td>\n",
       "      <td>503.046154</td>\n",
       "      <td>119.0</td>\n",
       "      <td>0.650056</td>\n",
       "      <td>492.504444</td>\n",
       "      <td>-0.999017</td>\n",
       "      <td>519.366667</td>\n",
       "      <td>-0.022189</td>\n",
       "      <td>1.833333</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>1.625000</td>\n",
       "      <td>3.113636</td>\n",
       "      <td>2.454545</td>\n",
       "      <td>5</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.515284</td>\n",
       "      <td>1.883117</td>\n",
       "      <td>0.077336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-2.328517</td>\n",
       "      <td>325.250694</td>\n",
       "      <td>104.0</td>\n",
       "      <td>-5.071762</td>\n",
       "      <td>371.577778</td>\n",
       "      <td>-2.279165</td>\n",
       "      <td>305.532812</td>\n",
       "      <td>0.011413</td>\n",
       "      <td>1.359538</td>\n",
       "      <td>116.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>1.450000</td>\n",
       "      <td>3.764706</td>\n",
       "      <td>2.393939</td>\n",
       "      <td>2</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>8.164103</td>\n",
       "      <td>2.696970</td>\n",
       "      <td>0.069153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-10.814640</td>\n",
       "      <td>343.483333</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-10.814640</td>\n",
       "      <td>343.483333</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.787107</td>\n",
       "      <td>151.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.027778</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>3.424242</td>\n",
       "      <td>3.459016</td>\n",
       "      <td>1</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.619048</td>\n",
       "      <td>9.423868</td>\n",
       "      <td>3.360656</td>\n",
       "      <td>-0.055941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.491273</td>\n",
       "      <td>359.260131</td>\n",
       "      <td>124.0</td>\n",
       "      <td>1.278975</td>\n",
       "      <td>379.585000</td>\n",
       "      <td>5.204547</td>\n",
       "      <td>332.385833</td>\n",
       "      <td>0.026114</td>\n",
       "      <td>1.071171</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.684211</td>\n",
       "      <td>1.315789</td>\n",
       "      <td>2.538462</td>\n",
       "      <td>2.902439</td>\n",
       "      <td>2</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>8.250000</td>\n",
       "      <td>2.463415</td>\n",
       "      <td>0.006012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.909116</td>\n",
       "      <td>347.736111</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-0.092374</td>\n",
       "      <td>338.680000</td>\n",
       "      <td>-1.791743</td>\n",
       "      <td>391.090000</td>\n",
       "      <td>0.016509</td>\n",
       "      <td>0.283660</td>\n",
       "      <td>107.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.888889</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>2.727273</td>\n",
       "      <td>2.850000</td>\n",
       "      <td>1</td>\n",
       "      <td>31.777778</td>\n",
       "      <td>0.676471</td>\n",
       "      <td>7.480769</td>\n",
       "      <td>1.950000</td>\n",
       "      <td>0.099516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.914004</td>\n",
       "      <td>403.415873</td>\n",
       "      <td>118.0</td>\n",
       "      <td>-1.449836</td>\n",
       "      <td>417.814444</td>\n",
       "      <td>-0.646226</td>\n",
       "      <td>388.513542</td>\n",
       "      <td>0.013595</td>\n",
       "      <td>1.465797</td>\n",
       "      <td>35.0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.018182</td>\n",
       "      <td>1.054545</td>\n",
       "      <td>4.444444</td>\n",
       "      <td>2.701493</td>\n",
       "      <td>1</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.363395</td>\n",
       "      <td>1.925373</td>\n",
       "      <td>-0.091453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.207459</td>\n",
       "      <td>357.486957</td>\n",
       "      <td>117.0</td>\n",
       "      <td>2.853368</td>\n",
       "      <td>356.220000</td>\n",
       "      <td>0.983248</td>\n",
       "      <td>347.200000</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>1.411798</td>\n",
       "      <td>121.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.838710</td>\n",
       "      <td>1.887097</td>\n",
       "      <td>4.240506</td>\n",
       "      <td>3.353160</td>\n",
       "      <td>4</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>9.528604</td>\n",
       "      <td>2.416357</td>\n",
       "      <td>0.050845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30 rows × 160 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    night_duration slope  night_duration mean  night_duration breakpoint  \\\n",
       "0              -0.164093           421.995238                      119.0   \n",
       "1              -2.059699           428.346296                      115.0   \n",
       "2              -1.617033           417.591176                      104.0   \n",
       "3              -1.324367           451.678205                      112.0   \n",
       "4               3.220315           452.329167                      112.0   \n",
       "5              -0.591977           350.504645                      108.0   \n",
       "6               0.252891           502.914286                      123.0   \n",
       "7              -1.081539           427.941481                      116.0   \n",
       "8              -1.148476           467.779667                      133.0   \n",
       "9              -0.519706           406.429861                      106.0   \n",
       "10             -2.421907           458.003333                       35.0   \n",
       "11             -0.707225           452.577778                      131.0   \n",
       "12             -1.341172           372.851064                      103.0   \n",
       "13             -0.726586           474.776190                      105.0   \n",
       "14             -0.341595           478.411111                      104.0   \n",
       "15             -2.499837           412.676667                      130.0   \n",
       "16             -1.982719           437.020833                       35.0   \n",
       "17             -6.311211           489.455556                       35.0   \n",
       "18             -0.974266           444.091667                      108.0   \n",
       "19             -1.066047           466.454955                      114.0   \n",
       "20             -0.763500           440.705556                      115.0   \n",
       "21             -1.142165           398.853546                      104.0   \n",
       "22              0.175806           545.935714                       35.0   \n",
       "23              0.595732           503.046154                      119.0   \n",
       "24             -2.328517           325.250694                      104.0   \n",
       "25            -10.814640           343.483333                       35.0   \n",
       "26             -0.491273           359.260131                      124.0   \n",
       "27              0.909116           347.736111                      121.0   \n",
       "28             -0.914004           403.415873                      118.0   \n",
       "29              0.207459           357.486957                      117.0   \n",
       "\n",
       "    night_duration pre slope  night_duration pre mean  \\\n",
       "0                   4.020310               431.862500   \n",
       "1                  -1.166852               468.026667   \n",
       "2                   7.012367               471.046667   \n",
       "3                  -4.400206               465.770000   \n",
       "4                  -1.304891               413.399306   \n",
       "5                  -4.377251               366.001333   \n",
       "6                  -1.101175               488.478333   \n",
       "7                   1.226258               447.687222   \n",
       "8                   0.167124               485.322381   \n",
       "9                   1.850716               420.232639   \n",
       "10                  0.000000                 0.000000   \n",
       "11                  0.129302               461.690000   \n",
       "12                  1.908154               453.731667   \n",
       "13                 -0.471886               494.968333   \n",
       "14                 -4.244512               496.958889   \n",
       "15                 -3.308664               431.166111   \n",
       "16                  0.000000                 0.000000   \n",
       "17                  0.000000                 0.000000   \n",
       "18                  0.781878               468.135000   \n",
       "19                 -0.879715               487.188333   \n",
       "20                  0.552346               465.644444   \n",
       "21                  1.223729               430.610000   \n",
       "22                  0.000000                 0.000000   \n",
       "23                  0.650056               492.504444   \n",
       "24                 -5.071762               371.577778   \n",
       "25                  0.000000                 0.000000   \n",
       "26                  1.278975               379.585000   \n",
       "27                 -0.092374               338.680000   \n",
       "28                 -1.449836               417.814444   \n",
       "29                  2.853368               356.220000   \n",
       "\n",
       "    night_duration post slope  night_duration post mean  \\\n",
       "0                   -1.448899                400.164286   \n",
       "1                   -1.709380                393.633333   \n",
       "2                    0.099656                393.591429   \n",
       "3                   -1.679831                435.131481   \n",
       "4                   -1.210151                532.116667   \n",
       "5                    1.041262                341.203810   \n",
       "6                   -1.454870                512.903333   \n",
       "7                   -1.082490                392.161538   \n",
       "8                   -3.901980                422.122619   \n",
       "9                    1.574997                386.098485   \n",
       "10                  -2.421907                458.003333   \n",
       "11                  -1.755000                419.206667   \n",
       "12                   0.329594                355.362963   \n",
       "13                   1.389038                444.551852   \n",
       "14                   0.798306                470.617544   \n",
       "15                   8.013225                345.802083   \n",
       "16                  -1.982719                437.020833   \n",
       "17                  -6.311211                489.455556   \n",
       "18                   2.503125                417.600000   \n",
       "19                   0.794196                439.437500   \n",
       "20                   0.225477                425.076087   \n",
       "21                   1.573797                378.611290   \n",
       "22                   0.175806                545.935714   \n",
       "23                  -0.999017                519.366667   \n",
       "24                  -2.279165                305.532812   \n",
       "25                 -10.814640                343.483333   \n",
       "26                   5.204547                332.385833   \n",
       "27                  -1.791743                391.090000   \n",
       "28                  -0.646226                388.513542   \n",
       "29                   0.983248                347.200000   \n",
       "\n",
       "    noise while studying slope  noise while studying mean  \\\n",
       "0                    -0.012393                   0.963071   \n",
       "1                    -0.004352                   1.114056   \n",
       "2                     0.006630                   1.269486   \n",
       "3                     0.000120                   1.452299   \n",
       "4                    -0.082633                   0.578431   \n",
       "5                     0.007990                   1.200926   \n",
       "6                     0.006853                   1.057366   \n",
       "7                     0.002002                   0.898692   \n",
       "8                     0.004033                   0.941023   \n",
       "9                     0.005175                   1.432985   \n",
       "10                    0.022534                   0.970043   \n",
       "11                   -0.075664                   0.277277   \n",
       "12                    0.003608                   1.398376   \n",
       "13                   -0.022732                   0.800024   \n",
       "14                    0.017793                   0.725473   \n",
       "15                    0.014461                   1.209935   \n",
       "16                    0.075800                   0.892072   \n",
       "17                   -0.002431                   1.237266   \n",
       "18                    0.012010                   1.000449   \n",
       "19                    0.003249                   0.983238   \n",
       "20                   -0.001783                   1.099539   \n",
       "21                    0.025000                   0.500000   \n",
       "22                    0.003892                   0.747952   \n",
       "23                   -0.022189                   1.833333   \n",
       "24                    0.011413                   1.359538   \n",
       "25                    0.001493                   0.787107   \n",
       "26                    0.026114                   1.071171   \n",
       "27                    0.016509                   0.283660   \n",
       "28                    0.013595                   1.465797   \n",
       "29                    0.003891                   1.411798   \n",
       "\n",
       "    noise while studying breakpoint  ...  sleep_quality  sleep_rested  \\\n",
       "0                             122.0  ...       3.350000      1.850000   \n",
       "1                             131.0  ...       3.052632      1.526316   \n",
       "2                             117.0  ...       3.000000      1.000000   \n",
       "3                              35.0  ...       0.000000      0.000000   \n",
       "4                              35.0  ...       3.066667      1.066667   \n",
       "5                             114.0  ...       3.181818      1.181818   \n",
       "6                              88.0  ...       0.000000      0.000000   \n",
       "7                             123.0  ...       2.820513      1.230769   \n",
       "8                             146.0  ...       3.400000      1.150000   \n",
       "9                             116.0  ...       3.440000      1.440000   \n",
       "10                            135.0  ...       3.000000      1.187500   \n",
       "11                             35.0  ...       3.035088      1.210526   \n",
       "12                            120.0  ...       2.294118      2.500000   \n",
       "13                            135.0  ...       3.076923      1.769231   \n",
       "14                            143.0  ...       3.346939      2.428571   \n",
       "15                            115.0  ...       3.615385      1.000000   \n",
       "16                             35.0  ...       2.750000      1.187500   \n",
       "17                            108.0  ...       2.800000      1.400000   \n",
       "18                             88.0  ...       2.416667      1.416667   \n",
       "19                             88.0  ...       3.428571      1.107143   \n",
       "20                            149.0  ...       2.644444      1.311111   \n",
       "21                             35.0  ...       2.694444      1.472222   \n",
       "22                            148.0  ...       3.100000      1.000000   \n",
       "23                             35.0  ...       2.750000      1.625000   \n",
       "24                            116.0  ...       2.650000      1.450000   \n",
       "25                            151.0  ...       3.027778      1.666667   \n",
       "26                             35.0  ...       3.684211      1.315789   \n",
       "27                            107.0  ...       2.888889      1.333333   \n",
       "28                             35.0  ...       3.018182      1.054545   \n",
       "29                            121.0  ...       2.838710      1.887097   \n",
       "\n",
       "    people_interacted_with  stress_level  row  total conscientiousness  \\\n",
       "0                 3.500000      3.125000    4                32.000000   \n",
       "1                 2.800000      3.066667    3                32.000000   \n",
       "2                 3.074074      2.232558    2                32.000000   \n",
       "3                 4.333333      3.875000    4                30.000000   \n",
       "4                 3.250000      3.773585    1                28.000000   \n",
       "5                 2.767442      2.333333    6                34.000000   \n",
       "6                 3.000000      3.666667    6                30.000000   \n",
       "7                 3.133333      3.320755    1                31.000000   \n",
       "8                 2.055556      2.766667    1                31.000000   \n",
       "9                 3.230769      2.054054    1                31.000000   \n",
       "10                4.400000      2.900000    5                34.000000   \n",
       "11                3.025000      2.280374    1                34.000000   \n",
       "12                2.451613      1.142857    1                36.000000   \n",
       "13                1.954545      2.625000    4                29.000000   \n",
       "14                3.487179      2.527473    3                33.000000   \n",
       "15                2.482759      3.400000    5                33.000000   \n",
       "16                4.080000      2.512821    6                29.000000   \n",
       "17                3.071429      3.150000    1                31.777778   \n",
       "18                2.125000      2.480000    5                30.000000   \n",
       "19                3.764706      3.533333    3                30.000000   \n",
       "20                4.214286      3.500000    6                33.000000   \n",
       "21                3.906250      2.586207    4                31.000000   \n",
       "22                3.428571      2.000000    5                31.777778   \n",
       "23                3.113636      2.454545    5                32.000000   \n",
       "24                3.764706      2.393939    2                34.000000   \n",
       "25                3.424242      3.459016    1                35.000000   \n",
       "26                2.538462      2.902439    2                28.000000   \n",
       "27                2.727273      2.850000    1                31.777778   \n",
       "28                4.444444      2.701493    1                31.000000   \n",
       "29                4.240506      3.353160    4                35.000000   \n",
       "\n",
       "    Class Attendance  positive affect    stress  positive affect post slope  \n",
       "0           0.607843         9.971429  2.250000                    0.085333  \n",
       "1           0.812500         9.000000  2.066667                   -0.022219  \n",
       "2           1.000000         8.651376  1.860465                    0.058258  \n",
       "3           0.622807         9.333333  3.375000                    0.700450  \n",
       "4           0.000000         9.993197  3.245283                    0.310667  \n",
       "5           0.545455         8.863636  1.800000                    0.058047  \n",
       "6           1.000000        10.000000  3.000000                    1.171226  \n",
       "7           0.650000         9.706070  2.396226                   -0.017630  \n",
       "8           0.000000         8.965174  2.233333                    0.090601  \n",
       "9           0.782609         8.176768  2.162162                    0.016784  \n",
       "10          0.370370         9.075472  1.500000                    0.114005  \n",
       "11          0.958333         9.359331  2.130841                   -0.029593  \n",
       "12          0.666667        11.260465  2.857143                   -0.019592  \n",
       "13          0.427083        11.182390  1.875000                    0.026501  \n",
       "14          0.000000        11.942708  1.978022                    0.012697  \n",
       "15          0.629630         7.119048  2.490909                    0.100732  \n",
       "16          0.555556         7.658683  1.948718                   -0.029317  \n",
       "17          0.500000         9.572917  2.650000                   -0.009404  \n",
       "18          0.433333         9.434066  1.840000                    0.018106  \n",
       "19          0.657407         9.850340  2.933333                   -0.102190  \n",
       "20          0.621212         9.900000  2.469697                    0.038915  \n",
       "21          0.559524         8.304795  2.896552                   -0.106482  \n",
       "22          0.000000         8.390244  2.000000                    0.006684  \n",
       "23          1.000000         8.515284  1.883117                    0.077336  \n",
       "24          0.854167         8.164103  2.696970                    0.069153  \n",
       "25          0.619048         9.423868  3.360656                   -0.055941  \n",
       "26          0.000000         8.250000  2.463415                    0.006012  \n",
       "27          0.676471         7.480769  1.950000                    0.099516  \n",
       "28          0.000000         9.363395  1.925373                   -0.091453  \n",
       "29          0.666667         9.528604  2.416357                    0.050845  \n",
       "\n",
       "[30 rows x 160 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "behavioral_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_df(behavioral_df): \n",
    "    # create numpy arrays for learning\n",
    "    features = behavioral_df.values\n",
    "    target = grades['spring_gpa'].values\n",
    "\n",
    "    # check how many features are used (the GPA paper had 193)\n",
    "    print('features shape: {}'.format(features.shape))\n",
    "    print('target shape: {}'.format(target.shape))\n",
    "    \n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_r_p(behavioral_df): \n",
    "    # create a dataframe for containing correlation and p value for each variable\n",
    "    stats_df = pd.DataFrame()\n",
    "\n",
    "    # loop through all columns \n",
    "    for column in behavioral_df.columns: \n",
    "        # calculate correlation and p-value\n",
    "        try: \n",
    "            stats = scipy.stats.pearsonr(behavioral_df[column], grades['spring_gpa'])\n",
    "        except: \n",
    "            print(column)\n",
    "            continue\n",
    "\n",
    "        # add the calculated stats to the dataframe.\n",
    "        new = pd.DataFrame({'stat': [column], 'correlation': [stats[0]], 'p-value': [stats[1]]})\n",
    "        stats_df = stats_df.append(new, ignore_index = True)\n",
    "\n",
    "    # making a df of significant values to reduce noise for models \n",
    "    sig_stats = stats_df[stats_df['p-value'] < .01]\n",
    "    sig_df = behavioral_df[sig_stats['stat'].values]\n",
    "    sig_features = sig_df.values\n",
    "    return sig_features, sig_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features shape: (30, 160)\n",
      "target shape: (30,)\n"
     ]
    }
   ],
   "source": [
    "features, target = features_from_df(behavioral_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_features, sig_stats = extract_r_p(behavioral_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stat</th>\n",
       "      <th>correlation</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>activity while studying mean</td>\n",
       "      <td>-0.570090</td>\n",
       "      <td>0.001006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>activity while studying post mean</td>\n",
       "      <td>-0.595778</td>\n",
       "      <td>0.000513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>activity inference slope</td>\n",
       "      <td>-0.602225</td>\n",
       "      <td>0.000430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>activity inference mean</td>\n",
       "      <td>-0.649109</td>\n",
       "      <td>0.000104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>activity inference pre mean</td>\n",
       "      <td>-0.463997</td>\n",
       "      <td>0.009801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>activity inference post slope</td>\n",
       "      <td>-0.511609</td>\n",
       "      <td>0.003856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>activity inference post mean</td>\n",
       "      <td>-0.698006</td>\n",
       "      <td>0.000018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>day activity inference slope</td>\n",
       "      <td>-0.522365</td>\n",
       "      <td>0.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>day activity inference mean</td>\n",
       "      <td>-0.607890</td>\n",
       "      <td>0.000366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>day activity inference pre mean</td>\n",
       "      <td>-0.476522</td>\n",
       "      <td>0.007764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>day activity inference post mean</td>\n",
       "      <td>-0.643194</td>\n",
       "      <td>0.000126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>evening activity inference slope</td>\n",
       "      <td>-0.629662</td>\n",
       "      <td>0.000193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>evening activity inference mean</td>\n",
       "      <td>-0.573070</td>\n",
       "      <td>0.000933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>evening activity inference post slope</td>\n",
       "      <td>-0.599906</td>\n",
       "      <td>0.000458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>evening activity inference post mean</td>\n",
       "      <td>-0.705356</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>night activity inference slope</td>\n",
       "      <td>-0.656463</td>\n",
       "      <td>0.000082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>night activity inference mean</td>\n",
       "      <td>-0.631343</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>night activity inference pre mean</td>\n",
       "      <td>-0.493204</td>\n",
       "      <td>0.005617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>night activity inference post slope</td>\n",
       "      <td>-0.523982</td>\n",
       "      <td>0.002959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>night activity inference post mean</td>\n",
       "      <td>-0.696279</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>average indoor mobility slope</td>\n",
       "      <td>-0.574311</td>\n",
       "      <td>0.000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>average indoor mobility mean</td>\n",
       "      <td>-0.635214</td>\n",
       "      <td>0.000162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>average indoor mobility post slope</td>\n",
       "      <td>-0.525836</td>\n",
       "      <td>0.002842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>average indoor mobility post mean</td>\n",
       "      <td>-0.696220</td>\n",
       "      <td>0.000019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>day indoor mobility slope</td>\n",
       "      <td>-0.531805</td>\n",
       "      <td>0.002490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>day indoor mobility mean</td>\n",
       "      <td>-0.610845</td>\n",
       "      <td>0.000337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>day indoor mobility post slope</td>\n",
       "      <td>-0.470400</td>\n",
       "      <td>0.008709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>day indoor mobility post mean</td>\n",
       "      <td>-0.657268</td>\n",
       "      <td>0.000079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>evening indoor mobility slope</td>\n",
       "      <td>-0.552567</td>\n",
       "      <td>0.001544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>evening indoor mobility mean</td>\n",
       "      <td>-0.537267</td>\n",
       "      <td>0.002202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>evening indoor mobility post slope</td>\n",
       "      <td>-0.524466</td>\n",
       "      <td>0.002928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>evening indoor mobility post mean</td>\n",
       "      <td>-0.677928</td>\n",
       "      <td>0.000039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>night indoor mobility slope</td>\n",
       "      <td>-0.511521</td>\n",
       "      <td>0.003863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>night indoor mobility mean</td>\n",
       "      <td>-0.500879</td>\n",
       "      <td>0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>night indoor mobility post slope</td>\n",
       "      <td>-0.549997</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>night indoor mobility post mean</td>\n",
       "      <td>-0.609504</td>\n",
       "      <td>0.000350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      stat  correlation   p-value\n",
       "15            activity while studying mean    -0.570090  0.001006\n",
       "20       activity while studying post mean    -0.595778  0.000513\n",
       "63                activity inference slope    -0.602225  0.000430\n",
       "64                 activity inference mean    -0.649109  0.000104\n",
       "67             activity inference pre mean    -0.463997  0.009801\n",
       "68           activity inference post slope    -0.511609  0.003856\n",
       "69            activity inference post mean    -0.698006  0.000018\n",
       "70            day activity inference slope    -0.522365  0.003065\n",
       "71             day activity inference mean    -0.607890  0.000366\n",
       "74         day activity inference pre mean    -0.476522  0.007764\n",
       "76        day activity inference post mean    -0.643194  0.000126\n",
       "77        evening activity inference slope    -0.629662  0.000193\n",
       "78         evening activity inference mean    -0.573070  0.000933\n",
       "82   evening activity inference post slope    -0.599906  0.000458\n",
       "83    evening activity inference post mean    -0.705356  0.000013\n",
       "84          night activity inference slope    -0.656463  0.000082\n",
       "85           night activity inference mean    -0.631343  0.000183\n",
       "88       night activity inference pre mean    -0.493204  0.005617\n",
       "89     night activity inference post slope    -0.523982  0.002959\n",
       "90      night activity inference post mean    -0.696279  0.000019\n",
       "91           average indoor mobility slope    -0.574311  0.000904\n",
       "92            average indoor mobility mean    -0.635214  0.000162\n",
       "96      average indoor mobility post slope    -0.525836  0.002842\n",
       "97       average indoor mobility post mean    -0.696220  0.000019\n",
       "98               day indoor mobility slope    -0.531805  0.002490\n",
       "99                day indoor mobility mean    -0.610845  0.000337\n",
       "103         day indoor mobility post slope    -0.470400  0.008709\n",
       "104          day indoor mobility post mean    -0.657268  0.000079\n",
       "105          evening indoor mobility slope    -0.552567  0.001544\n",
       "106           evening indoor mobility mean    -0.537267  0.002202\n",
       "110     evening indoor mobility post slope    -0.524466  0.002928\n",
       "111      evening indoor mobility post mean    -0.677928  0.000039\n",
       "112            night indoor mobility slope    -0.511521  0.003863\n",
       "113             night indoor mobility mean    -0.500879  0.004813\n",
       "117       night indoor mobility post slope    -0.549997  0.001640\n",
       "118        night indoor mobility post mean    -0.609504  0.000350"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import LeaveOneOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_prediction(model, features, target): \n",
    "    \"\"\"\n",
    "    given input model, features, and target, prints scores of the model on the features\n",
    "    \"\"\"\n",
    "    # split into training and testing data\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(features, target, random_state = 2)\n",
    "    \n",
    "    # fit the model\n",
    "    model.fit(X_train, Y_train)\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # print scores\n",
    "    print('test score: %2f' % model.score(X_test, Y_test))\n",
    "    print('training score: %2f' % model.score(X_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_predictions(): \n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # scale the data to mean 0 and variance 1\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # loop through some models and see how they score without much parameter tuning. \n",
    "    linear_models = [Lasso(), Lasso(alpha = .01), Ridge(), Ridge(alpha = .01)]\n",
    "    models = ['Lasso, alpha = 1', 'Lasso, alpha = .01', 'Ridge, alpha = 1', 'Ridge, alpha = .01']\n",
    "\n",
    "    for index in range(len(linear_models)):\n",
    "        print(models[index])\n",
    "        basic_prediction(linear_models[index], scaled_features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results lead me to believe I'm overfitting the data, I'll try some parameter tuning in a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating mean absolute error of the model\n",
    "def loo_r2(predictor, features, target):\n",
    "    \"\"\"\n",
    "    inputs: predictor - machine learning model to fit on the data\n",
    "            scaled_features - features to fit the model with\n",
    "            target - goal predictions \n",
    "    uses leave one out cross validation to output the mean absolute error of the model. \n",
    "    also returns mean absolute error of the model\n",
    "    \"\"\"\n",
    "    \n",
    "    # split the data leaving one user out in each split\n",
    "    leave_one = LeaveOneOut()\n",
    "    loo = leave_one.split(X = scaled_features, y = target)\n",
    "\n",
    "    # loop through all the splits\n",
    "    results = []\n",
    "    for train_index, test_index in loo:\n",
    "        X_train = features[train_index]\n",
    "        Y_train = target[train_index]\n",
    "        X_test = features[test_index]\n",
    "        Y_test = target[test_index]\n",
    "        \n",
    "        # add the predicted and actual value to the results list\n",
    "        predictor.fit(X_train, Y_train)\n",
    "            \n",
    "        prediction = float(predictor.predict(X_test))\n",
    "        # gpas cannot be greater than four, so cap predictions at 4\n",
    "        if prediction > 4: \n",
    "            predicton = 4\n",
    "\n",
    "        results.append([prediction, float(Y_test)])\n",
    "        \n",
    "        \n",
    "    # R^2 = (1-u/v) where u is residual sum of squares (y_true - y_pred) and v is the total sum of squares \n",
    "    # (y_true - y_true.mean()) ** 2 taken from \n",
    "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "    u = 0\n",
    "    v = 0\n",
    "    total = 0\n",
    "    \n",
    "    for res in results: \n",
    "        u += (res[1] - res[0])**2\n",
    "        v += (res[1] - target.mean())**2\n",
    "        total += abs(res[1]-res[0])\n",
    "        \n",
    "    total = total/len(results)\n",
    "    \n",
    "    return 1-u/v, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grades_gridsearch(model, features, target, param_grid): \n",
    "    \"\"\"\n",
    "    inputs: model - machine learning model to fit on the data (either ridge or lasso)\n",
    "            features - features to fit the model with\n",
    "            target - goal predictions \n",
    "    outputs: the optimal alpha value for the model\n",
    "    \"\"\"\n",
    "    # use a gridsearch with cross validation to find the best parameters\n",
    "\n",
    "    # find the best parameters on the X_train and Y_train set, then use them on the test set\n",
    "    #X_train, X_test, Y_train, Y_test = train_test_split(features, target, random_state = 1)\n",
    "\n",
    "    # implementing the grid search\n",
    "    grid = GridSearchCV(model, param_grid, cv = 5)\n",
    "\n",
    "    grid.fit(features, target)\n",
    "\n",
    "    # print scoring and parameters\n",
    "    print(\"Best parameters, scaling: {}\".format(grid.best_params_))\n",
    "    print(\"Best cross-validation score, scaling: {:.2f}\".format(grid.best_score_))\n",
    "    \n",
    "    return grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(): \n",
    "    # adding a polynomial pipeline and testing it. Is not helpful. \n",
    "    pipe = Pipeline([('polynomial', PolynomialFeatures()), \n",
    "                     ('scaler', StandardScaler()), ('lasso', Lasso(alpha = best_alpha))])\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(scaled_features, target, random_state = 1)\n",
    "\n",
    "    pipe.fit(X_train, Y_train)\n",
    "\n",
    "    print('score with polynomial features added: {:.4f}'.format(pipe.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model clearly still needs more work. While it occasionally performs well, I need to work more on the tuning of the hyperparameters, since lasso especially is very sensitive to those, and add some more data. If the model continues to overfit, I will need to impose more regularization or lean more on only using significant features when fitting the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso, alpha = 1\n",
      "test score: -0.084240\n",
      "training score: 0.000000\n",
      "Lasso, alpha = .01\n",
      "test score: 0.187551\n",
      "training score: 0.997712\n",
      "Ridge, alpha = 1\n",
      "test score: -0.007717\n",
      "training score: 0.999920\n",
      "Ridge, alpha = .01\n",
      "test score: -0.011553\n",
      "training score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "naive_predictions()\n",
    "scaled_features = StandardScaler().fit_transform(features)\n",
    "scaled_sig_features = StandardScaler().fit_transform(sig_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters, scaling: {'alpha': 0.1}\n",
      "Best cross-validation score, scaling: -0.33\n"
     ]
    }
   ],
   "source": [
    "linear_param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "best_lasso_alpha = grades_gridsearch(Lasso(max_iter = 100000), scaled_features, target, linear_param_grid)['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters, scaling: {'alpha': 100}\n",
      "Best cross-validation score, scaling: -0.19\n"
     ]
    }
   ],
   "source": [
    "best_ridge_alpha = grades_gridsearch(Ridge(max_iter = 100000), scaled_features, target, linear_param_grid)['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters, scaling: {'alpha': 1}\n",
      "Best cross-validation score, scaling: -0.45\n"
     ]
    }
   ],
   "source": [
    "best_sig_lasso_alpha = grades_gridsearch(Lasso(max_iter = 100000), scaled_sig_features, target, linear_param_grid)['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters, scaling: {'alpha': 100}\n",
      "Best cross-validation score, scaling: -0.23\n"
     ]
    }
   ],
   "source": [
    "best_sig_ridge_alpha = grades_gridsearch(Ridge(max_iter = 100000), scaled_sig_features, target, linear_param_grid)['alpha']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso r^2 (leave one out cv): 0.196\n",
      "Ridge r^2 (leave one out cv): 0.216\n",
      "Lasso mae (leave one out cv): 0.504\n",
      "Ridge mae (leave one out cv): 0.501\n"
     ]
    }
   ],
   "source": [
    "print('Lasso r^2 (leave one out cv): {:.3f}'.format(loo_r2(Lasso(max_iter = 1000000, \n",
    "                                                                 alpha = best_lasso_alpha), scaled_features, target)[0]))\n",
    "print('Ridge r^2 (leave one out cv): {:.3f}'.format(loo_r2(Ridge(max_iter = 1000000, \n",
    "                                                              alpha = best_ridge_alpha), scaled_features, target)[0]))\n",
    "\n",
    "print('Lasso mae (leave one out cv): {:.3f}'.format(loo_r2(Lasso(max_iter = 1000000, \n",
    "                                                                 alpha = best_lasso_alpha), scaled_features, target)[1]))\n",
    "print('Ridge mae (leave one out cv): {:.3f}'.format(loo_r2(Ridge(max_iter = 1000000, \n",
    "                                                              alpha = best_ridge_alpha), scaled_features, target)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sig Lasso r^2 (leave one out cv): -0.070\n",
      "Sig Ridge r^2 (leave one out cv): -0.041\n",
      "Sig Lasso mae (leave one out cv): 0.564\n",
      "Sig Ridge mae (leave one out cv): 0.561\n"
     ]
    }
   ],
   "source": [
    "print('Sig Lasso r^2 (leave one out cv): {:.3f}'.format(loo_r2(Lasso(max_iter = 1000000, \n",
    "                                                            alpha = best_sig_lasso_alpha), sig_features, target)[0]))\n",
    "print('Sig Ridge r^2 (leave one out cv): {:.3f}'.format(loo_r2(Ridge(max_iter = 1000000, \n",
    "                                                             alpha = best_sig_ridge_alpha), sig_features, target)[0]))\n",
    "\n",
    "print('Sig Lasso mae (leave one out cv): {:.3f}'.format(loo_r2(Lasso(max_iter = 1000000, \n",
    "                                                            alpha = best_sig_lasso_alpha), sig_features, target)[1]))\n",
    "print('Sig Ridge mae (leave one out cv): {:.3f}'.format(loo_r2(Ridge(max_iter = 1000000, \n",
    "                                                             alpha = best_sig_ridge_alpha), sig_features, target)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting with Unsupervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def plot_tsne(features):\n",
    "    \"\"\"\n",
    "    given input features plots tsne with 5 perplexities\n",
    "    \"\"\"\n",
    "    scaled_grades_features = StandardScaler().fit_transform(features)\n",
    "    \n",
    "    for perp in [5, 10, 30, 100]: \n",
    "        transformed_features = TSNE(n_components=2, perplexity = perp).fit_transform(scaled_grades_features)\n",
    "\n",
    "        #plt.scatter(x = transformed_features[:, 0], y = transformed_features[:, 1])\n",
    "\n",
    "        to_plot = pd.DataFrame({'comp_1': transformed_features[:, 0], 'comp_2': transformed_features[:, 1],\n",
    "                                'grades': target, 'deans_list': grades['deans_list']})\n",
    "\n",
    "        sns.relplot(x = 'comp_1', y = 'comp_2', size = 'grades', data = to_plot, sizes = (20, 200), hue = 'deans_list')\n",
    "        plt.title('perplexity {}'.format(perp))\n",
    "        plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There isn't an evident pattern in any of the tsne graphs, so i'll try a PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there is appears to be no pattern in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(sig_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tsne(scaled_sig_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a noticeable pattern in the signifcant features tsne with perplexity 30, but not a noticeable pattern in the scaled significant features or the total features set. As a result, in further analysis, I will use the the unscaled signficant features to in another grid search and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def plot_pca(features):\n",
    "    \"\"\"\n",
    "    given input features plots a principal component analysis (first two components)\n",
    "    \"\"\"\n",
    "    # keep the first two principal components of the data\n",
    "    pca = PCA(n_components=2)\n",
    "    pg_transformed = pca.fit_transform(features)\n",
    "    pg_df = pd.DataFrame(data = pg_transformed, columns = ['PC_1', 'PC_2'])\n",
    "    pg_df['deans_list'] = grades['deans_list']\n",
    "    #print(\"component 1 weights: \", pca.components_[0])\n",
    "    #print(\"component 2 weights: \", pca.components_[1])\n",
    "    print(\"variance explained: \", pca.explained_variance_ratio_)\n",
    "    sns.relplot(x = 'PC_1', y = 'PC_2', hue = 'deans_list', kind = 'scatter', data = pg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pca(sig_features)\n",
    "plt.title('Principal Component Analysis on significant features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_lasso_alpha = grades_gridsearch(Lasso(max_iter = 100000), sig_features, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempting to make the model with only the 7 features used in the final GPA paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_df = extract_7_features_df()\n",
    "# make a numpy array for the features\n",
    "small_features = feat_df.values\n",
    "feat_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our array of gpas for the corresponding ordered uids\n",
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_small_features = MinMaxScaler().fit_transform(small_features)\n",
    "#scaled_small_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_small_alpha = grades_gridsearch(Ridge(max_iter = 100000), scaled_small_features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Scaled Lasso r^2 (leave one out cv): {:.3f}'.format(loo_r2(Lasso(max_iter = 1000000, \n",
    "                                                             alpha = best_small_alpha), scaled_small_features, target)[0]))\n",
    "\n",
    "print('Scaled Lasso mae (leave one out cv): {:.3f}'.format(loo_r2(Lasso(max_iter = 1000000, \n",
    "                                                            alpha = best_small_alpha), scaled_small_features, target)[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying a gradient boosted regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_gridsearch(features, target, scoring): \n",
    "    \"\"\"\n",
    "    inputs: features, target and scoring preferences\n",
    "    outputs: a dictionary of the best parameters for the gradient boosting regressor. \n",
    "    \"\"\"\n",
    "    param_grid = {'learning_rate': [.01, .1, 1, 10, 100],\n",
    "                  'max_depth': [1, 3, 5, 7, 9]}\n",
    "    \n",
    "    return grades_gridsearch(GradientBoostingRegressor(loss = scoring), features, target, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i choose to use lad because it gives me the fewest overflow messages. \n",
    "best_params = gradient_boosting_gridsearch(features, target, 'lad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_grad = GradientBoostingRegressor(loss = 'lad', learning_rate = best_params['learning_rate'], \n",
    "                                    max_depth = best_params['max_depth'])\n",
    "\n",
    "error = loo_r2(new_grad, scaled_features, target)\n",
    "\n",
    "print('Gradient Boosting Regressor r^2 (leave one out cv): {:.3f}'.format(error[0]))\n",
    "print('Gradient Boosting Regressor mae (leave one out cv): {:.3f}'.format(error[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient boosting regressor isn't any better than the lasso or ridge regressors right now. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
